# -*- coding: utf-8 -*-
"""Network for a-maps

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/107haNcbC_BydB0GBhUpTDz70kUNKBIpX
"""

from google.colab import drive

drive.mount('/content/gdrive')

!ls

!ls "/content/gdrive/My Drive/GW_Dataset"

import matplotlib.pyplot as plt
import seaborn as sns

import keras
from keras.models import Sequential
from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout 
from keras.preprocessing.image import ImageDataGenerator

from tensorflow.keras.optimizers import Adam

from sklearn.metrics import classification_report,confusion_matrix

import tensorflow as tf

import cv2
import os

import numpy as np

pip install split-folders

import splitfolders
splitfolders.ratio('/content/gdrive/MyDrive/GW_Dataset', output="Output", seed=1337, ratio=(.8, 0.1,0.1))

import os
import numpy as np
import shutil

# Creating Train / Val / Test folders (One time use)
root_dir = '/content/gdrive/MyDrive/GW_Dataset'

# os.makedirs("Output" +'/train')
# os.makedirs("Output" +'/val')
# os.makedirs("Output" +'/test')


# val_ratio = 0.15
# test_ratio = 0.15

# Creating partitions of the data after shuffeling

src = root_dir # Folder to copy images from

allFileNames = os.listdir(src)
np.random.shuffle(allFileNames)
train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                           [int(len(allFileNames)* 0.7), 
                                                               int(len(allFileNames)* 0.85)])


train_FileNames = [src+'/'+ name for name in train_FileNames.tolist()]
val_FileNames = [src+'/' + name for name in val_FileNames.tolist()]
test_FileNames = [src+'/' + name for name in test_FileNames.tolist()]

print('Total images: ', len(allFileNames))
print('Training: ', len(train_FileNames))
print('Validation: ', len(val_FileNames))
print('Testing: ', len(test_FileNames))

# Copy-pasting images
for name in train_FileNames:
    shutil.copy(name, "Output/train")

for name in val_FileNames:
    shutil.copy(name, "Output/val")

for name in test_FileNames:
    shutil.copy(name, "Output/test")

#print(train_FileNames)
train_files = os.listdir("Output/train")
train_y = []
for name in train_files:
  name = name[9]+"."+name[11]
  name = float(name)
  train_y.append(name)
print(train_y)

test_files = os.listdir("Output/test")
test_y = []
for name in test_files:
  name = name[9]+"."+name[11]
  name = float(name)
  test_y.append(name)
print(test_y)

val_files = os.listdir("Output/val")
val_y = []
for name in val_files:
  name = name[9]+"."+name[11]
  name = float(name)
  val_y.append(name)
print(val_y)

#Data Preparation

img_size = 128
def get_data(data_dir, labels):
    data = [] 
    for index, img in enumerate(os.listdir(data_dir)):
        try:
          img_arr = cv2.imread(os.path.join(data_dir, img), 0)#[..., ::-1]
          img_arr = img_arr[..., np.newaxis]
        
          #resized_arr = cv2.resize(img_arr, (img_size, img_size)) # Reshaping images to preferred size
          data.append([img_arr, labels[index]])
        except Exception as e:
          print(e)
    return np.array(data)

img_arr = cv2.imread('Output/train/01338_HG_4_3_0.50_n01_n30.png', 0)
print(img_arr)

train = get_data('Output/train', train_y)
test = get_data('Output/test', test_y)
val = get_data('Output/val', val_y)

plt.figure(figsize = (5,5))
#plt.imshow(train[11][0])
plt.title(train[11][0])

train[1][0].shape

from collections import Counter
import tensorflow as tf
x_train = []
y_train = []
x_val = []
y_val = []
x_test = []
y_test = []

for feature, label in train:
  x_train.append(feature)
  y_train.append(str(int(label*10)))
y_train_cat=tf.keras.utils.to_categorical(y_train)


for feature, label in val:
  x_val.append(feature)
  y_val.append(str(int(label*10)))
y_val_cat=tf.keras.utils.to_categorical(y_val)

for feature, label in test:
  x_test.append(feature)
  y_test.append(str(int(label*10)))
y_test_cat=tf.keras.utils.to_categorical(y_test)

# Normalize the data
x_train = np.array(x_train) / 255
x_val = np.array(x_val) / 255
x_test = np.array(x_test) / 255

x_train.reshape(-1, img_size, img_size, 1)
y_train_cat = np.array(y_train_cat)

x_val.reshape(-1, img_size, img_size, 1)
y_val_cat = np.array(y_val_cat)

print(y_val_cat.shape)
print(y_train_cat.shape)



x_train.shape

model = Sequential()
model.add(Conv2D(32,(3, 3),padding="same", activation="relu", input_shape=(128, 128, 1)))
model.add(MaxPool2D())

model.add(Conv2D(32, (3, 3), padding="same", activation="relu"))
model.add(MaxPool2D())

model.add(Conv2D(64, (3, 3), padding="same", activation="relu"))
model.add(MaxPool2D())
model.add(Dropout(0.4))

model.add(Flatten())
model.add(Dense(128,activation="relu"))
model.add(Dense(56, activation="softmax"))

model.summary()

opt = Adam(learning_rate=0.0001)
model.compile(optimizer = opt , loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True) , metrics = ['accuracy'])

history = model.fit(x_train,y_train_cat,epochs = 50 , validation_data = (x_val, y_val_cat))



model.evaluate(x_test,y_test_cat)

predict = model.predict(x_test)
classes = np.argmax(predict, axis = 1)
print(classes)

y_test

root_dir = '/content/gdrive/My Drive/GW_Input_Data_2'

val_files = os.listdir(root_dir)
val_new_y = []

for img_file in val_files:
  img_file = img_file[3]+"."+img_file[5]
  img_file = float(img_file)
  val_new_y.append(img_file)

def get_val_data(data_dir, labels):
  data = []
  for index, img in enumerate (os.listdir(data_dir)):
    try:
      img_arr = cv2.imread(os.path.join(data_dir, img))[...,::-1] #convert BGR to RGB format
      #resized_arr = cv2.resize(img_arr, (img_size, img_size)) # Reshaping images to preferred size
      data.append([img_arr, labels[index]])
    except Exception as e:
      print(e)
  return np.array(data)

val_data = get_val_data(root_dir, val_new_y)

val_data

new_x_val = []
new_y_val = []

for feature, label in val_data:
  new_x_val.append(feature)
  new_y_val.append(str(int(label*10)))
y_test_cat=tf.keras.utils.to_categorical(new_y_val)

#normalising

new_x_val = np.array(new_x_val) / 255
new_y_val = np.array(new_y_val)

model.evaluate(new_x_val,y_test_cat)

y_test_cat

new_y_val

model.predict_classes(new_x_val)

pip install LightPipes

pip install numpy

import numpy as np
import os

from LightPipes import *
import matplotlib.pyplot as plt

from PIL import Image, ImageChops
import random
np.random.seed(23234)

# Number of images to be generated
Ntot=10

# Defining variables: Wavelength of 1064 Nanometer, 40*40 mm2 sqaure grid with 128x128 pixels
wavelength = 1064*nm
size = 40*mm
Npix = 128
w0=3*mm
LG=False

# Maximum number of modes to be generated 
mode_max=6
mode_m=np.random.randint(0,mode_max,Ntot)
mode_n=np.random.randint(0,mode_max,Ntot)

# Noise distribution
mean = 0
std_list = [0.25, 0.5, 1]
sigma = np.random.choice(std_list, Ntot)

# Offset 
x_offset = np.random.randint(-30,30, Ntot)
y_offset = np.random.randint(-30,30, Ntot)

#The Begin command generates a field with amplitude 1.0 and phase zero, a plane wave. 
#So, all the 128x128 elements of array contain the complex number: 1.0 + j0.0
F0=Begin(size,wavelength,Npix)

catalog = open("catalog.txt","w")
for num in range(Ntot):
   nn=mode_n[num]
   mm=mode_m[num]
   x_off=x_offset[num] 
   y_off=y_offset[num] 
   F1=GaussBeam(F0, w0, LG=LG, n=nn, m=mm)
   Iimg=Intensity(F1,1) #Intensity is calculated and normalized to 255 (2 -> 255, 1 -> 1.0, 0 -> not normalized)
   
   gauss = np.random.normal(mean,sigma[num],Iimg.shape)
   gauss_img = gauss.reshape(Iimg.shape)
   noisyIimg = Iimg + gauss_img
   print(noisyIimg)
  #  offset_im = ImageChops.offset(noisyIimg, x_off, y_off)
   
   offset_image = noisyIimg.shape[0] + x_off, noisyIimg.shape[1] + y_off
   offset_im[:-x_offset, :-y_offset] 
   print(offset_im)
  
   filename=f'HG_{nn}_{mm}_noise_{sigma[num]}_off_{x_off}_{y_off}.png'
   print(filename)

   # kk = f'HG_{nn}_{mm}_{wavelength}'
   # fname = f'{kk}.png'
   #plt.imsave(filename, offset_im, cmap='gray')
   # im = Image.open(fname)
   # offset_im = ImageChops.offset(im, x_off, y_off)
   

   # os.rename(fname, filename)
   # offset_im.save(filename)
   catalog.write("%d %d %d %.2f %d %d \n"%(num+1,nn,mm,sigma[num],x_off,y_off))

catalog.close()

offset_im.shape

import LightPipes as lp
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image, ImageChops


def noisy(image: np.ndarray=np.zeros((50,50,3))):
      
          nor_image = image//np.mean(image)
          nor_image = image 
          mean = 0
          std_list = [0.25,0.5, 0.75]
          std = np.random.choice(std_list, 1)
          sigma = std[0]
          gauss = np.random.normal(mean,sigma,nor_image.shape)
          gauss_img = gauss.reshape(nor_image.shape)
          noisy = nor_image + gauss_img
          return noisy, sigma
    
    # else:
    #     return None, None



wavelengths = [wave*lp.mm for wave in range(1064,1065)]
size = 40*lp.mm 
N = 128
w0=3*lp.mm
LG=False
n_max=6;m_max=6


for wavelength in wavelengths:
    F=lp.Begin(size,wavelength,N)
    for m in range(m_max):
        for n in range(n_max):
            
            F=lp.GaussBeam(F, w0, LG=LG, n=n, m=m)
            I=lp.Intensity(F,1) #Normalised to 1
            Phi=lp.Phase(F)            
                      
            
            x_offset = np.random.randint(1,30)
            y_offset = np.random.randint(1,30)
        
            offset_image = np.zeros((I.shape[0]+x_offset, I.shape[1]+y_offset))
            if np.random.randint(1,10) % 2 == 0:
                offset_image[x_offset:, y_offset:]=I
            else:
                offset_image[:-x_offset, :-y_offset]=I
            offset_noisy_image, sigma = noisy(offset_image)

            print(offset_image)

            filename = f'HG_{n}_{m}_{sigma}_off_{x_offset}_{y_offset}.png'
            plt.imsave(filename, offset_noisy_image, cmap='gray')

