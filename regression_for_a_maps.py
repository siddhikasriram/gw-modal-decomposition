# -*- coding: utf-8 -*-
"""regression_for_a_maps

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fKzcy_sTjN9AzudY13jpoSOOfwi8spqp
"""

pip install LightPipes

import numpy as np
import os
from LightPipes import *
import matplotlib.pyplot as plt
from PIL import Image, ImageChops
import random
from numpy.lib.index_tricks import index_exp
np.random.seed(7561)

# Number of images to be generated
Ntot=1000

# Defining variables: Wavelength of 1064 Nanometer, 40*40 mm2 sqaure grid with 128x128 pixels
wavelength = 1064*nm
size = 40*mm
Npix = 128
w0=3*mm
LG=False

# Maximum number of modes to be generated 
mode_max=6
mode_m=np.random.randint(0,mode_max,Ntot)
mode_n=np.random.randint(0,mode_max,Ntot)

# Noise distribution
mean = 0
std_list = [0.25, 0.50, 1.00]
sigma = np.random.choice(std_list, Ntot)

# Offset 
x_offset = np.random.randint(-30,30, Ntot)
y_offset = np.random.randint(-30,30, Ntot)

#The Begin command generates a field with amplitude 1.0 and phase zero, a plane wave. 
#So, all the 128x128 elements of array contain the complex number: 1.0 + j0.0
F0=Begin(size,wavelength,Npix)

# Catalog is the record of all the images and it's parameters generated
catalog = open("catalog.txt","w")
for num in range(Ntot):
   nn=mode_n[num]
   mm=mode_m[num]
   x_off=x_offset[num] 
   y_off=y_offset[num] 
   F1=GaussBeam(F0, w0, LG=LG, n=nn, m=mm)
   Iimg=Intensity(F1,1) #Intensity is calculated and normalized to 255 (2 -> 255, 1 -> 1.0, 0 -> not normalized)
   
   # Adding Noise
   gauss = np.random.normal(mean,sigma[num],Iimg.shape)
   gauss_img = gauss.reshape(Iimg.shape)
   noisyIimg = Iimg + gauss_img
   
   # Index creates unique IDs for each image 
   index = str(int(num +1)).zfill(5)
   noiseFile = f'{index}_HG_{nn}_{mm}'
   fname = f'{noiseFile}.png'
   plt.imsave(fname, noisyIimg, cmap='gray')

   # Adding offset 
   im = Image.open(fname)
   offset_im = ImageChops.offset(im, x_off, y_off)

   # Naming the images before saving
   sigma_value = format(sigma[num], '.2f')
   catalog.write("%s %d %d %s %d %d \n"%(index,nn,mm,sigma_value,x_off,y_off))

   if x_off < 0 and y_off < 0:
      x_off = str(abs(x_off)).zfill(2)
      y_off = str(abs(y_off)).zfill(2)
      filename=f'{noiseFile}_{sigma_value}_n{x_off}_n{y_off}.png'
   elif x_off >= 0 and y_off >= 0:
      x_off = str(x_off).zfill(2)
      y_off = str(y_off).zfill(2)
      filename=f'{noiseFile}_{sigma_value}_p{x_off}_p{y_off}.png'
   elif x_off >= 0 and y_off < 0:
      x_off = str(x_off).zfill(2)
      y_off = str(abs(y_off)).zfill(2)
      filename=f'{noiseFile}_{sigma_value}_p{x_off}_n{y_off}.png'
   elif x_off < 0 and y_off >= 0:
      x_off = str(abs(x_off)).zfill(2)
      y_off = str(y_off).zfill(2)
      filename=f'{noiseFile}_{sigma_value}_n{x_off}_p{y_off}.png'

   os.rename(fname, filename)
   offset_im.save(filename)
   
catalog.close()

import pandas
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasRegressor
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import tensorflow as tf
import keras
from keras.models import Sequential
from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout 
from keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import classification_report, confusion_matrix
import cv2
import os
import shutil
import numpy as np
from keras.models import Model
from keras.layers import Flatten
from keras.models import Sequential
from keras.layers.core import Dense
from keras.layers import concatenate
from keras.layers.core import Dropout
from keras.callbacks import TensorBoard
from keras.models import model_from_yaml
from keras.layers.convolutional import Conv2D
from keras.layers.convolutional import MaxPooling2D
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.optimizers import Adam

"""# New Section"""

# load dataset
#root_dir = '/home/siddhika/dataset/'

os.makedirs("Output" +'/train')
os.makedirs("Output" +'/val')
os.makedirs("Output" +'/test')


val_ratio = 0.15
test_ratio = 0.15

# Creating partitions of the data after shuffeling

#src = root_dir # Folder to copy images from

import glob
allFileNames = glob.glob('*.png')
np.random.shuffle(allFileNames)
train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),
                                                           [int(len(allFileNames)* 0.7), 
                                                               int(len(allFileNames)* 0.85)])

print('Total images: ', len(allFileNames))
print('Training: ', len(train_FileNames))
print('Validation: ', len(val_FileNames))
print('Testing: ', len(test_FileNames))

# Copy-pasting images
for name in train_FileNames:
    shutil.copy(name, "Output/train")

for name in val_FileNames:
    shutil.copy(name, "Output/val")

for name in test_FileNames:
    shutil.copy(name, "Output/test")

name = train_FileNames[0]
print(name)
temm = name[9]
temn = name[11]
xoff = name[18:21]
yoff = name[22:25]

def labelling (fileName):
  yParam = []
  for image in fileName:
    labels = []
    labels.append(int(image[9]))
    labels.append(int(image[11]))
    if image[18] == 'p':
      labels.append(int("+"+image[19:21]))
    
    else:
      labels.append(int("-"+image[19:21]))
      
    if image[22] == 'p':
      labels.append(int("+"+image[23:25]))
      
    else:
      labels.append(int("-"+image[23:25]))
    yParam.append(labels)
  yParam = np.array([np.array(x) for x in yParam])
  return yParam

yTrain = labelling(train_FileNames)
yTest = labelling(test_FileNames)
yVal = labelling(val_FileNames)

len(yTest)

type(yVal[0])

img_size = 128
def getdata(data_path, y):
  print(len(os.listdir(data_path)))
  data=[]
  for index, img in enumerate(os.listdir(data_path)):
    img_arr = cv2.imread(os.path.join(data_path, img), 0)#[..., ::-1]
    img_arr = img_arr[..., np.newaxis]

    #resized_arr = cv2.resize(img_arr, (img_size, img_size)) # Reshaping images to preferred size
    data.append(img_arr)
  
  return data

training = getdata("Output/train", yTrain)
testing = getdata("Output/test", yTest)
validation = getdata("Output/val", yVal)

print(len(validation))

x_train = []
y_train = []
x_val = []
y_val = []
x_test = []
y_test = []

# Normalize the data
x_train = np.array(training) / 255
x_val = np.array(validation) / 255
x_test = np.array(testing) / 255

x_train.reshape(-1, img_size, img_size, 1)
x_test.reshape(-1, img_size, img_size, 1)
x_val.reshape(-1, img_size, img_size, 1)

len(x_train)

x_test.shape #doubt

model = Sequential()
model.add(Conv2D(128,(3, 3),padding="same", activation="relu", input_shape=(128, 128, 1)))
model.add(BatchNormalization(axis= -1))
model.add(MaxPool2D(pool_size = (2,2)))

model.add(Conv2D(64, (3, 3), padding="same", activation="relu"))
model.add(BatchNormalization(axis= -1))
model.add(MaxPool2D(pool_size = (2,2)))
model.add(Dropout(0.2))

model.add(Conv2D(32, (3, 3), padding="same", activation="relu"))
model.add(BatchNormalization(axis= -1))
model.add(MaxPool2D(pool_size = (2,2)))
model.add(Dropout(0.3))

model.add(Flatten())
model.add(Dense(8, activation='relu'))
model.add(BatchNormalization(axis= -1))
model.add(Dense(4, activation='relu'))

model.summary()

opt = Adam(learning_rate=0.0001)
model.compile(optimizer = opt , loss ='mean_squared_error' , metrics = ['accuracy'])

history = model.fit(x_train,yTrain,epochs = 5)

preds = model.predict(x_test)

preds

yTrain

